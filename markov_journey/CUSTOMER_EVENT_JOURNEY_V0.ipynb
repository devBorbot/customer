{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "rwvj2b464sebzk2uwcmm",
   "authorId": "4469548203817",
   "authorName": "RYAN.BORMAN@HDSUPPLY.COM",
   "authorEmail": "Ryan.Borman@hdsupply.com",
   "sessionId": "dfab1569-b97a-45e2-b62e-645c7d1c5fd8",
   "lastEditTime": 1766438735615
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "import pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom collections import defaultdict, Counter, defaultdict\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ec001ca0-bf9a-4415-ba74-24dccedd181d",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# =====================================================================================\n# CONFIG / PARAMETERS (TUNE THESE)\n# =====================================================================================\n\nNUM_CUSTOMERS = 500\nSTART_DATE = datetime(2025, 10, 1)\nEND_DATE = datetime(2025, 11, 30)\nCONVERSION_PROB = 0.25  # probability a customer converts in synthetic data\n\nEVENT_STATE_MAPPING = {\n    \"email_open\": \"Email\",\n    \"paid_search_click\": \"Paid_Search\",\n    \"social_ad_click\": \"Social\",\n    \"organic_search\": \"Organic\",\n    \"homepage_visit\": \"Homepage\",\n    \"product_page_view\": \"Product_Page\",\n    \"pricing_page_view\": \"Pricing_Page\",\n    \"cart_add\": \"Cart\",\n    \"purchase\": \"Conversion\",\n    \"support_contact\": \"Support\",\n}\n\nABSORBING_STATES = [\"Conversion\", \"Null\"]\nSTART_STATE = \"Start\"\n\n# ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "044f481c-8c76-4b82-8a9e-9d3574f52cf1",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# =====================================================================================\n# STEP 1 – GENERATE SYNTHETIC EVENT DATA\n# =====================================================================================\n\ndef generate_synthetic_events(\n    num_customers=NUM_CUSTOMERS,\n    start_date=START_DATE,\n    end_date=END_DATE,\n    conversion_prob=CONVERSION_PROB,\n    event_state_mapping=EVENT_STATE_MAPPING,\n):\n    np.random.seed(42)\n    random.seed(42)\n\n    customer_ids = [f\"CUST_{str(i).zfill(4)}\" for i in range(1, num_customers + 1)]\n    all_events = []\n\n    for customer_id in customer_ids:\n        num_events = random.randint(1, 10)\n\n        customer_start = start_date + timedelta(days=random.randint(0, 30))\n        timestamps = sorted(\n            [\n                customer_start\n                + timedelta(\n                    days=random.randint(0, 30),\n                    hours=random.randint(0, 23),\n                    minutes=random.randint(0, 59),\n                )\n                for _ in range(num_events)\n            ]\n        )\n\n        converted = random.random() < conversion_prob\n\n        # Build a plausible journey\n        awareness_events = [\n            \"email_open\",\n            \"paid_search_click\",\n            \"social_ad_click\",\n            \"organic_search\",\n        ]\n        journey_events = [random.choice(awareness_events)]\n\n        for _ in range(num_events - 2 if num_events > 2 else 0):\n            r = random.random()\n            if r < 0.4:\n                journey_events.append(\"homepage_visit\")\n            elif r < 0.4 + 0.5:\n                journey_events.append(\"product_page_view\")\n            elif r < 0.4 + 0.5 + 0.3:\n                journey_events.append(\"pricing_page_view\")\n            elif r < 0.4 + 0.5 + 0.3 + 0.2:\n                journey_events.append(\"cart_add\")\n            else:\n                journey_events.append(random.choice([\"email_open\", \"support_contact\"]))\n\n        journey_events = journey_events[:num_events]\n\n        if converted and num_events > 1:\n            journey_events.append(\"purchase\")\n\n        for ts, event_type in zip(timestamps, journey_events):\n            all_events.append(\n                {\n                    \"customer_id\": customer_id,\n                    \"event_time\": ts,\n                    \"event_type\": event_type,\n                    \"state\": event_state_mapping[event_type],\n                }\n            )\n\n    events_df = pd.DataFrame(all_events).sort_values([\"customer_id\", \"event_time\"])\n    events_df[\"event_time\"] = pd.to_datetime(events_df[\"event_time\"])\n    return events_df\n\n\n# =====================================================================================\n# STEP 2 – BUILD CUSTOMER PATHS\n# =====================================================================================\n\ndef build_customer_paths(events_df, start_state=START_STATE):\n    \"\"\"\n    Build per-customer state paths and a summary DataFrame for Markov journey analysis.\n\n    This function constructs a path of states for each customer by:\n    - Prepending a global `start_state`.\n    - Appending a terminal `\"Null\"` state if the last observed state is not `\"Conversion\"`.\n\n    It returns both:\n    - `paths`: a list of lists where each inner list is the full state sequence for a customer\n      (including `start_state` and possibly `\"Null\"`).\n    - `paths_df`: a summary DataFrame with one row per customer containing:\n        * `customer_id`: the customer's identifier.\n        * `path_length`: the number of states in the constructed path.\n        * `path`: a human-readable string of the path joined by `\" → \"`.\n        * `converted`: whether the customer's final observed state equals `\"Conversion\"`.\n        * `num_states`: the count of unique *intermediate* states excluding\n          `{start_state, \"Null\", \"Conversion\"}`.\n\n    Parameters\n    ----------\n    events_df : pandas.DataFrame\n        A DataFrame of customer events containing at least:\n        - `customer_id` (hashable): identifier used to group events.\n        - `state` (str): the journey state at each event.\n        The rows for each `customer_id` are assumed to be in chronological order. If not,\n        sort before calling, e.g. `events_df.sort_values([\"customer_id\", \"event_time\"])`.\n    start_state : str, optional\n        The state used to prepend each customer's path (default: `START_STATE`).\n\n    Returns\n    -------\n    paths : list[list[str]]\n        The list of per-customer state sequences, each beginning with `start_state`\n        and ending with `\"Conversion\"` or `\"Null\"`.\n    paths_df : pandas.DataFrame\n        Summary per customer with columns:\n        `customer_id`, `path_length`, `path`, `converted`, `num_states`.\n\n    Raises\n    ------\n    KeyError\n        If `events_df` does not contain `customer_id` or `state` columns.\n    IndexError\n        If a customer group has zero rows (i.e., `events_df` has customers with no events),\n        since the function accesses `states[-1]`.\n\n    Notes\n    -----\n    - A terminal `\"Null\"` state is appended only when the last observed state is not `\"Conversion\"`.\n    - `num_states` excludes `{start_state, \"Null\", \"Conversion\"}` to focus on intermediate\n      journey states relevant for transition attribution.\n\n    Examples\n    --------\n    >>> df = pd.DataFrame({\n    ...     \"customer_id\": [1, 1, 2],\n    ...     \"state\": [\"Email\", \"Conversion\", \"Social\"]\n    ... })\n    >>> paths, paths_df = build_customer_paths(df, start_state=\"Start\")\n    >>> paths\n    [['Start', 'Email', 'Conversion'], ['Start', 'Social', 'Null']]\n    >>> paths_df[[\"customer_id\", \"path\", \"converted\"]]\n       customer_id                      path  converted\n    0            1      Start → Email → Conversion       True\n    1            2           Start → Social → Null      False\n    \"\"\"\n    paths = []\n    rows = []\n\n    for cust_id, group in events_df.groupby(\"customer_id\"):\n        states = group[\"state\"].tolist()\n        path = [start_state] + states\n        if states[-1] != \"Conversion\":\n            path.append(\"Null\")\n\n        paths.append(path)\n        rows.append(\n            {\n                \"customer_id\": cust_id,\n                \"path_length\": len(path),\n                \"path\": \" → \".join(path),\n                \"converted\": states[-1] == \"Conversion\",\n                \"num_states\": len(set(path) - {start_state, \"Null\", \"Conversion\"}),\n            }\n        )\n\n    paths_df = pd.DataFrame(rows)\n    return paths, paths_df\n\n# =====================================================================================\n# STEP 3 – COUNT TRANSITIONS & BUILD TRANSITION MATRIX\n# =====================================================================================\n\ndef count_transitions(paths):\n    \"\"\"\n    Count state-to-state transitions across all customer paths.\n\n    Parameters\n    ----------\n    paths : list of list of str\n        Each inner list is a sequence of states for a single customer.\n\n    Returns\n    -------\n    transition_counts : dict of tuple[str, str] to int\n        Python-native mapping (from_state, to_state) -> count.\n    all_states : set of str\n        Set of all unique states.\n    transition_records : list of dict\n        JSON-ready list with keys: 'from', 'to', 'count'.\n\n    Notes\n    -----\n    - `transition_counts` uses tuple keys which are not JSON-serializable.\n      Use `transition_records` when emitting to JSON or UI layers.\n    \"\"\"\n    transition_counts = defaultdict(int)\n    all_states = set()\n    for path in paths:\n        all_states.update(path)\n        for i in range(len(path) - 1):\n            transition_counts[(path[i], path[i + 1])] += 1\n            \n    # Build JSON-friendly records\n    transition_records = [{\"from\": src, \"to\": dst, \"count\": cnt}\n                          for (src, dst), cnt in transition_counts.items()]\n\n    return dict(transition_counts), all_states, transition_records\n\ndef build_transition_matrix(transition_counts, all_states, absorbing_states=ABSORBING_STATES, smoothing=0.0):\n    \"\"\"\n    Build a row-normalized Markov transition probability matrix.\n\n    This function constructs a square transition matrix over the provided `all_states`.\n    It first initializes the matrix with a uniform `smoothing` value, then adds\n    observed transition counts from `transition_counts`. Each row is normalized\n    to sum to 1, producing a row-stochastic matrix of transition probabilities.\n    Finally, any state specified in `absorbing_states` is converted to an absorbing\n    state by setting its row to all zeros except a 1 on the diagonal.\n\n    Parameters\n    ----------\n    transition_counts : dict[tuple[str, str], int]\n        Mapping from (from_state, to_state) pairs to observed transition counts.\n        Usually produced by `count_transitions(paths)`.\n    all_states : set[str] or list[str]\n        The universe of states to include in the matrix. The output matrix will be\n        square over these states. States are sorted lexicographically to define the\n        row/column order.\n    absorbing_states : Iterable[str], optional\n        States that should be treated as absorbing (i.e., once entered, the process\n        remains there with probability 1). Defaults to `ABSORBING_STATES`.\n    smoothing : float, optional\n        Additive smoothing value applied as a baseline to every (from_state, to_state)\n        cell before counts are added. Use `0.0` for no smoothing, or a small value\n        (e.g. `1e-6` or `1.0`) to avoid zero rows and improve numerical stability.\n\n    Returns\n    -------\n    trans_mat : pandas.DataFrame\n        A row-stochastic transition probability matrix with:\n        - Index: states (sorted).\n        - Columns: states (sorted).\n        - Values: probabilities in [0, 1].\n        Absorbing states (if present) will have rows with a 1 on the diagonal and 0 elsewhere.\n\n    Notes\n    -----\n    - Row normalization is performed via `mat.div(mat.sum(axis=1), axis=0)`.\n      If a row sum is zero (e.g., no outgoing transitions and `smoothing == 0.0`),\n      the resulting row will contain `NaN`. Consider using a positive `smoothing`\n      or post-filling `NaN` as needed.\n    - The order of states in `trans_mat` is the sorted order of `all_states`.\n    - If a transition `(from_state, to_state)` appears in `transition_counts` but one of\n      the states is not in `all_states`, it will be ignored (only states within `all_states`\n      are represented).\n\n    Examples\n    --------\n    >>> transition_counts = {\n    ...     (\"Start\", \"Email\"): 3,\n    ...     (\"Email\", \"Conversion\"): 2,\n    ...     (\"Start\", \"Social\"): 1,\n    ...     (\"Social\", \"Null\"): 1,\n    ... }\n    >>> all_states = {\"Start\", \"Email\", \"Social\", \"Conversion\", \"Null\"}\n    >>> absorbing_states = {\"Conversion\", \"Null\"}\n    >>> trans_mat = build_transition_matrix(transition_counts, all_states,\n    ...                                     absorbing_states=absorbing_states,\n    ...                                     smoothing=0.0)\n    >>> trans_mat.loc[\"Start\"]\n    Email         0.75\n    Social        0.25\n    Conversion    0.00\n    Null          0.00\n    Name: Start, dtype: float64\n    >>> trans_mat.loc[\"Conversion\"]\n    Start         0.0\n    Email         0.0\n    Social        0.0\n    Conversion    1.0\n    Name: Conversion, dtype: float64\n    \"\"\"\n    states = sorted(all_states)\n    mat = pd.DataFrame(smoothing, index=states, columns=states)\n\n    for (from_s, to_s), c in transition_counts.items():\n        mat.loc[from_s, to_s] += c\n\n    trans_mat = mat.div(mat.sum(axis=1), axis=0)\n\n    for s in absorbing_states:\n        if s in trans_mat.index:\n            trans_mat.loc[s, :] = 0.0\n            trans_mat.loc[s, s] = 1.0\n\n    return trans_mat\n\n# =====================================================================================\n# STEP 4 – CONVERSION PROBABILITIES (ABSORBING MARKOV CHAIN)\n# =====================================================================================\n\ndef calculate_conversion_probabilities(transition_matrix, absorbing_states=ABSORBING_STATES):\n    \"\"\"\n    Compute per-state conversion probabilities in an absorbing Markov chain.\n\n    Given a row-stochastic transition matrix that includes transient states and\n    absorbing states (e.g., {\"Conversion\", \"Null\"}), this function calculates the\n    probability of eventual absorption into the **Conversion** state for each\n    transient state using the fundamental matrix of the chain.\n\n    The calculation follows the standard absorbing Markov chain formulation:\n    - Partition the transition matrix into submatrices:\n        * Q: transitions among transient states.\n        * R: transitions from transient states to absorbing states.\n    - Compute the fundamental matrix N = (I - Q)^{-1}. If (I - Q) is singular,\n      the Moore–Penrose pseudoinverse is used as a fallback.\n    - Absorption probabilities are given by B = N @ R.\n    - The conversion probability vector is extracted from the first column of B,\n      assuming the **first absorbing state** in `absorbing_states` is `\"Conversion\"`.\n\n    Parameters\n    ----------\n    transition_matrix : pandas.DataFrame\n        Row-stochastic transition probability matrix where both the index and columns\n        are states. Must include the absorbing states present in `absorbing_states`.\n        Rows should sum to ~1 (floating point tolerance), except for any missing data.\n    absorbing_states : Iterable[str], optional\n        Collection of absorbing states included in the transition matrix. The function\n        assumes the **first** item corresponds to `\"Conversion\"`, and will set:\n        - `conv_probs[\"Conversion\"] = 1.0`\n        - `conv_probs[\"Null\"] = 0.0`\n        by convention. Defaults to `ABSORBING_STATES`.\n\n    Returns\n    -------\n    conv_probs : pandas.Series\n        A Series indexed by states with the conversion probability for each state.\n        - Transient states: probability in [0, 1] of eventually reaching `\"Conversion\"`.\n        - `\"Conversion\"`: 1.0\n        - `\"Null\"` (or other non-conversion absorbing states): 0.0 (by convention).\n        The Series is sorted in descending order.\n\n    Raises\n    ------\n    KeyError\n        If any state in `absorbing_states` is not present in the transition matrix\n        index/columns.\n    ValueError\n        If there are no transient states (i.e., all states are absorbing), making\n        the computation undefined.\n\n    Notes\n    -----\n    - The function treats all states not in `absorbing_states` as **transient**.\n    - The fundamental matrix is computed as `N = inv(I - Q)`. If `(I - Q)` is singular,\n      `pinv(I - Q)` is used to produce a stable result.\n    - The first absorbing state is assumed to be `\"Conversion\"`. If your absorbing states\n      are ordered differently, adjust the extraction of `B[:, 0]` accordingly.\n    - If your absorbing set includes states other than `\"Conversion\"` and `\"Null\"`,\n      you may wish to set those entries in `conv_probs` explicitly to 0.0 (unless they\n      represent alternative forms of conversion).\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> import numpy as np\n    >>> states = [\"Start\", \"Email\", \"Social\", \"Conversion\", \"Null\"]\n    >>> tm = pd.DataFrame(0.0, index=states, columns=states)\n    >>> # Define some transitions (row-stochastic for transient states)\n    >>> tm.loc[\"Start\", [\"Email\", \"Social\"]] = [0.7, 0.3]\n    >>> tm.loc[\"Email\", [\"Conversion\", \"Null\"]] = [0.6, 0.4]\n    >>> tm.loc[\"Social\", [\"Email\", \"Null\"]] = [0.5, 0.5]\n    >>> # Absorbing rows\n    >>> tm.loc[\"Conversion\", \"Conversion\"] = 1.0\n    >>> tm.loc[\"Null\", \"Null\"] = 1.0\n    >>> conv_probs = calculate_conversion_probabilities(\n    ...     tm, absorbing_states=[\"Conversion\", \"Null\"]\n    ... )\n    >>> float(conv_probs[\"Start\"]) > 0\n    True\n    >>> conv_probs[\"Conversion\"] == 1.0 and conv_probs[\"Null\"] == 0.0\n    True\n    \"\"\"\n\n    import numpy as np\n\n    trans_states = [s for s in transition_matrix.index if s not in absorbing_states]\n    # Q[i, j] is the probability of transitioning from transient state i to transient state j\n    Q = transition_matrix.loc[trans_states, trans_states].values\n    # R[i, k] is the probability of transitioning from transient state i to absorbing state k\n    R = transition_matrix.loc[trans_states, absorbing_states].values\n\n    I = np.eye(len(trans_states))\n    # N[i, j] is the expected number of visits to transient state j starting from transient state i before absorption.\n    # It tells you how “sticky” certain states are in the journey\n    try:\n        N = np.linalg.inv(I - Q)\n    except np.linalg.LinAlgError:\n        N = np.linalg.pinv(I - Q) # If I - Q is singular (non-invertible)\n        \n    # B[i, k] is the probability of being absorbed into absorbing state k when starting from transient state i\n    B = N @ R\n    conv_probs = pd.Series(B[:, 0], index=trans_states)  # first absorbing is Conversion\n    conv_probs[\"Conversion\"] = 1.0\n    conv_probs[\"Null\"] = 0.0\n    return conv_probs.sort_values(ascending=False)\n\n# =====================================================================================\n# STEP 5 – REMOVAL EFFECT ATTRIBUTION\n# =====================================================================================\n\ndef calculate_removal_effect(transition_matrix, conversion_probs, paths, absorbing_states=ABSORBING_STATES):\n    \"\"\"\n    Estimate channel removal effects on total conversion using a Markov-chain reallocation heuristic.\n\n    This function quantifies the contribution of each non-absorbing, non-start channel by\n    measuring the drop in total expected conversions after virtually removing that channel\n    from the transition matrix. Removal is modeled in two steps:\n      1. **Redistribution of inbound probabilities to the removed channel**:\n         For each `from_state`, the probability that previously flowed to the removed channel\n         is set to 0 and redistributed proportionally across the remaining outgoing states\n         (preserving row-stochasticity).\n      2. **Channel becomes absorbing to \"Null\"**:\n         The removed channel's row is set to all zeros except a 1 on `\"Null\"`, making it an\n         absorbing non-conversion terminal.\n\n    After adjusting the transition matrix for each channel, the function recomputes\n    per-state conversion probabilities and compares the **baseline total expected conversions**\n    to the **new total**. The difference is recorded as the channel's removal effect score.\n\n    Parameters\n    ----------\n    transition_matrix : pandas.DataFrame\n        Row-stochastic transition probability matrix (index and columns are identical state sets).\n        Must include the `absorbing_states` (e.g., {\"Conversion\", \"Null\"}) and the `START_STATE`\n        (referenced globally). Absorbing rows should have a 1 on the diagonal and 0 elsewhere.\n    conversion_probs : pandas.Series\n        Baseline per-state probability of eventual absorption in `\"Conversion\"`. Typically produced\n        by `calculate_conversion_probabilities(transition_matrix, absorbing_states)`. Must include\n        entries for all transient states and absorbing states (with `\"Conversion\" = 1.0`, `\"Null\" = 0.0`).\n    paths : list[list[str]]\n        Customer paths as lists of states (including the `START_STATE` as the first element).\n        The function uses `path[1]` (the **first post-start state**) as the entry state when\n        summing conversion probabilities across customers. Paths with length ≤ 1 are ignored.\n    absorbing_states : Iterable[str], optional\n        The set of absorbing states present in the transition matrix. Defaults to `ABSORBING_STATES`\n        (expected to include `\"Conversion\"` and `\"Null\"`).\n\n    Returns\n    -------\n    percentages : pandas.Series\n        A Series of removal effect percentages per channel, normalized to sum to 100 when the\n        total removal effect is positive. If the total effect is non-positive, all percentages\n        are set to 0.0. Sorted in descending order.\n    scores : dict[str, float]\n        Raw removal effect scores per channel: `baseline_total - new_total` after removing the channel.\n        Larger positive values indicate a greater impact of the channel on total expected conversions.\n\n    Notes\n    -----\n    - **Baseline total** is computed as:\n      `sum(conversion_probs[path[1]] for path in paths if len(path) > 1)`,\n      i.e., the sum of conversion probabilities of first post-start states across customers.\n    - **Redistribution step** preserves each transient row's total probability mass\n      (except for the removed channel), by distributing the removed probability\n      proportionally to remaining outgoing probabilities.\n    - The removed channel is set to an **absorbing \"Null\"** state:\n      its row is zeroed and `\"Null\"` is set to 1.0 (if `\"Null\"` exists).\n    - The function excludes `START_STATE` and absorbing states from the set of channels\n      considered for removal.\n    - If the sum of all channel scores is ≤ 0, percentage outputs are all 0.0 to avoid\n      misleading normalization.\n\n    Examples\n    --------\n    >>> # Assume `tm` is a valid transition matrix with states including START_STATE, \"Conversion\", \"Null\".\n    >>> # Assume `paths` were generated by `build_customer_paths`.\n    >>> conv_probs = calculate_conversion_probabilities(tm, absorbing_states=[\"Conversion\", \"Null\"])\n    >>> pct, raw_scores = calculate_removal_effect(tm, conv_probs, paths, absorbing_states=[\"Conversion\", \"Null\"])\n    >>> pct.head()\n    Email      42.7\n    Social     31.4\n    Search     18.9\n    Display     7.0\n    dtype: float64\n\n    Edge Cases\n    ----------\n    - If a `from_state` has zero remaining outgoing probability after removing the channel\n      (i.e., all mass pointed to the removed channel), redistribution is skipped; the row\n      will sum to 0 for that state until normalization by `calculate_conversion_probabilities`.\n    - If `\"Null\"` is not present among `absorbing_states`/columns, the removed channel's row\n      becomes all zeros; consider adding `\"Null\"` or handling this explicitly upstream.\n    - Ensure `paths` contain at least one post-start state for meaningful totals; otherwise,\n      `baseline_total` and `new_total` may be 0, leading to zero percentages.\n    \"\"\"\n\n    channels = [s for s in transition_matrix.index if s not in (START_STATE, *absorbing_states)]\n    baseline_total = sum(conversion_probs[path[1]] for path in paths if len(path) > 1)\n    scores = {}\n\n    for ch in channels:\n        tm = transition_matrix.copy()\n\n        # redistribute transitions going to this channel\n        for from_s in tm.index:\n            if from_s == ch:\n                continue\n            removed = tm.loc[from_s, ch]\n            if removed <= 0:\n                continue\n            tm.loc[from_s, ch] = 0.0\n            remain_states = [s for s in tm.columns if s != ch]\n            remain_sum = tm.loc[from_s, remain_states].sum()\n            if remain_sum > 0:\n                tm.loc[from_s, remain_states] += removed * (tm.loc[from_s, remain_states] / remain_sum)\n\n        # channel becomes absorbing to Null\n        tm.loc[ch, :] = 0.0\n        if \"Null\" in tm.columns:\n            tm.loc[ch, \"Null\"] = 1.0\n\n        new_conv_probs = calculate_conversion_probabilities(tm, absorbing_states)\n        new_total = sum(new_conv_probs[path[1]] for path in paths if len(path) > 1)\n        scores[ch] = baseline_total - new_total\n\n    # Option B: zero-out negatives for the percentage view\n    positive_scores = {k: max(v, 0.0) for k, v in scores.items()}\n    pos_total = sum(positive_scores.values())\n    \n    if pos_total > 0:\n        pct = {k: v / pos_total * 100.0 for k, v in positive_scores.items()}\n    else:\n        pct = {k: 0.0 for k in scores}\n    \n    attribution_pct = pd.Series(pct).sort_values(ascending=False)\n    return attribution_pct, scores  # keep raw scores (including negatives) for diagnostics\n\n# =====================================================================================\n# STEP 6 – SIMPLE EDA & SAVES\n# =====================================================================================\n\ndef basic_eda(events_df, paths_df, transition_matrix, conversion_probs, attribution_pct):\n    print(\"\\n=== BASIC EDA ===\")\n    print(f\"Events: {len(events_df)}, Customers: {events_df['customer_id'].nunique()}\")\n    print(f\"States: {events_df['state'].nunique()}, Unique transitions: {len(transition_matrix)**2 - (transition_matrix==0).sum().sum()}\")\n    print(f\"Conversion rate: {paths_df['converted'].mean():.2%}\")\n    print(\"\\nState distribution:\")\n    print(events_df[\"state\"].value_counts())\n    print(\"\\nConversion probability by state:\")\n    print(conversion_probs)\n    print(\"\\nChannel attribution (%):\")\n    print(attribution_pct)\n\ndef save_all_outputs(\n    events_df,\n    paths_df,\n    transition_matrix,\n    conversion_probs,\n    transition_counts,\n    connectivity_df,\n    attribution_pct,\n    attribution_raw,\n):\n    # 01 events\n    events_df.to_csv(\"01_customer_events.csv\", index=False)\n\n    # 02 paths\n    paths_df.to_csv(\"02_customer_paths.csv\", index=False)\n\n    # 03 transition matrix\n    transition_matrix.to_csv(\"03_transition_matrix.csv\")\n\n    # 04 conversion probabilities\n    conv_out = pd.DataFrame(\n        {\n            \"State\": conversion_probs.index,\n            \"Conversion_Probability\": conversion_probs.values,\n            \"Conversion_Percentage\": conversion_probs.values * 100,\n        }\n    )\n    conv_out.to_csv(\"04_conversion_probabilities.csv\", index=False)\n\n    # 05 channel attribution\n    attr_df = pd.DataFrame(\n        {\n            \"Channel\": attribution_pct.index,\n            \"Attribution_Percentage\": attribution_pct.values,\n            \"Raw_Score\": [attribution_raw[c] for c in attribution_pct.index],\n        }\n    )\n    attr_df.to_csv(\"05_channel_attribution.csv\", index=False)\n\n    # 06 transition counts\n    tc_df = pd.DataFrame(\n        {\n            \"From_State\": [k[0] for k in transition_counts.keys()],\n            \"To_State\": [k[1] for k in transition_counts.keys()],\n            \"Count\": list(transition_counts.values()),\n        }\n    ).sort_values(\"Count\", ascending=False)\n    tc_df.to_csv(\"06_transition_counts.csv\", index=False)\n\n    # 07 connectivity\n    in_deg = (transition_matrix > 0).sum(axis=0)\n    out_deg = (transition_matrix > 0).sum(axis=1)\n    connectivity_df = pd.DataFrame(\n        {\n            \"State\": transition_matrix.index,\n            \"In_Degree\": in_deg.values,\n            \"Out_Degree\": out_deg.values,\n            \"Total_Connections\": in_deg.values + out_deg.values,\n        }\n    )\n    connectivity_df.to_csv(\"07_state_connectivity.csv\", index=False)\n\n    print(\"Saved CSV files 01–07 in current directory.\")\n\n# =====================================================================================\n# STEP 7 – INTERACTIVE HELPERS\n# =====================================================================================\n\ndef analyze_customer(customer_id, paths_df, events_df):\n\n    \"\"\"\n    Display a detailed view of a single customer's journey and event timeline.\n\n    This function retrieves the customer's path summary from `paths_df` and their\n    chronological event history from `events_df`. It prints:\n    - The full path as a string.\n    - Path length, conversion status, and number of unique intermediate states.\n    - A timestamped event timeline with event type and associated state.\n\n    Parameters\n    ----------\n    customer_id : str\n        The unique identifier of the customer to analyze.\n    paths_df : pandas.DataFrame\n        DataFrame containing per-customer path summaries, typically produced by\n        `build_customer_paths()`. Must include columns:\n        - `customer_id`\n        - `path` (string representation of the journey)\n        - `path_length` (int)\n        - `converted` (bool)\n        - `num_states` (int)\n    events_df : pandas.DataFrame\n        DataFrame of raw customer events, typically used to build paths. Must include:\n        - `customer_id`\n        - `event_time` (datetime-like)\n        - `event_type` (str)\n        - `state` (str)\n\n    Returns\n    -------\n    None\n        Prints the customer's path summary and event timeline to stdout. If the\n        customer ID is not found in `paths_df`, prints a message and returns early.\n\n    Notes\n    -----\n    - The event timeline is sorted by `event_time` in ascending order.\n    - If the customer has no recorded events or path, the function prints\n      `\"No data for {customer_id}\"` and exits.\n    - This function is intended for interactive exploration and does not return\n      structured data.\n\n    Examples\n    --------\n    >>> analyze_customer(\"CUST_0001\", paths_df, events_df)\n    === CUSTOMER CUST_0001 ===\n    Path: Start → Email → Conversion\n    Path length: 3, Converted: True, Unique states: 1\n\n    Event timeline:\n      2025-12-19 09:15:00 | Email Campaign       | Email\n      2025-12-19 09:30:00 | Purchase             | Conversion\n    \"\"\"\n\n    row = paths_df[paths_df[\"customer_id\"] == customer_id]\n    if row.empty:\n        print(f\"No data for {customer_id}\")\n        return\n    row = row.iloc[0]\n    print(f\"\\n=== CUSTOMER {customer_id} ===\")\n    print(f\"Path: {row['path']}\")\n    print(f\"Path length: {row['path_length']}, Converted: {row['converted']}, Unique states: {row['num_states']}\")\n    ev = events_df[events_df[\"customer_id\"] == customer_id].sort_values(\"event_time\")\n    print(\"\\nEvent timeline:\")\n    for _, e in ev.iterrows():\n        print(f\"  {e['event_time']} | {e['event_type']:20s} | {e['state']}\")\n\ndef state_transition_analysis(from_state, transition_matrix):\n    \"\"\"\n    Display outbound transition probabilities from a given state.\n\n    This function retrieves the row corresponding to `from_state` in the\n    transition matrix and prints all non-zero transitions to other states,\n    sorted in descending order of probability. Each transition is displayed\n    as a percentage for easy interpretation.\n\n    Parameters\n    ----------\n    from_state : str\n        The state from which outbound transitions should be analyzed.\n        Must exist in the index of `transition_matrix`.\n    transition_matrix : pandas.DataFrame\n        A row-stochastic transition probability matrix where both index and\n        columns represent states. Typically produced by `build_transition_matrix()`.\n\n    Returns\n    -------\n    None\n        Prints the transitions and their probabilities to stdout. If `from_state`\n        is not found in the matrix, prints an error message and exits.\n\n    Notes\n    -----\n    - Only transitions with probability > 0 are displayed.\n    - Probabilities are shown as percentages with two decimal places.\n    - This function is intended for interactive exploration and does not return\n      structured data.\n\n    Examples\n    --------\n    >>> state_transition_analysis(\"Email\", transition_matrix)\n    === Transitions from Email ===\n      → Conversion      : 60.00%\n      → Null            : 40.00%\n    \"\"\"\n    if from_state not in transition_matrix.index:\n        print(f\"State '{from_state}' not in matrix.\")\n        return\n    print(f\"\\n=== Transitions from {from_state} ===\")\n    row = transition_matrix.loc[from_state].sort_values(ascending=False)\n    for s, p in row[row > 0].items():\n        print(f\"  → {s:15s}: {p*100:5.2f}%\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f0618fd-073d-42aa-81a1-1d1884924ffd",
   "metadata": {
    "language": "python",
    "name": "cell4",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# =====================================================================================\n# MAIN EXECUTION\n# =====================================================================================\n\nif __name__ == \"__main__\":\n    print(\"Generating synthetic data...\")\n    events_df = generate_synthetic_events()\n    print(\"Building customer paths...\")\n    paths, paths_df = build_customer_paths(events_df)\n    print(\"Counting transitions...\")\n    transition_counts, all_states, transition_records = count_transitions(paths) # transition_counts exists but does not render in notebook\n    print(\"Building transition matrix...\")\n    transition_matrix = build_transition_matrix(transition_counts, all_states)\n    print(\"Computing conversion probabilities...\")\n    conversion_probs = calculate_conversion_probabilities(transition_matrix)\n    print(\"Computing removal-effect attribution...\")\n    attribution_pct, attribution_raw = calculate_removal_effect(\n        transition_matrix, conversion_probs, paths\n    )\n\n    in_deg = (transition_matrix > 0).sum(axis=0)\n    out_deg = (transition_matrix > 0).sum(axis=1)\n    connectivity_df = pd.DataFrame(\n        {\n            \"State\": transition_matrix.index,\n            \"In_Degree\": in_deg.values,\n            \"Out_Degree\": out_deg.values,\n            \"Total_Connections\": in_deg.values + out_deg.values,\n        }\n    )\n\n    basic_eda(events_df, paths_df, transition_matrix, conversion_probs, attribution_pct)\n\n    # Example interactive usage\n    print(\"\\nExample: analyze one customer:\")\n    analyze_customer(\"CUST_0002\", paths_df, events_df)\n\n    print(\"\\nExample: transitions from Email:\")\n    state_transition_analysis(\"Email\", transition_matrix)\n\n    print(\"\\nDone.\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "68319377-377b-44ad-9f22-0aa0e33c8804",
   "metadata": {
    "language": "python",
    "name": "cell6",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "events_df.head(10)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b482646a-fc60-4bcc-93aa-bda3543aaddd",
   "metadata": {
    "language": "python",
    "name": "cell7",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab45a130-bd52-48e3-9bb4-d786ed959b6f",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "55ecdfe5-4a4f-4c48-859e-c39bb392f593",
   "metadata": {
    "name": "CONV_PROB",
    "collapsed": false
   },
   "source": "## calculate_conversion_probabilities\nWhat does this function do in plain business terms?  \n\nIt answers:  \n**“If a customer starts in a certain state (like seeing an ad, visiting a webpage, or opening an email), what is the probability they will eventually convert?”**\n\nThink of your customer journey as a map of possible steps (states). Some steps are transient (e.g., browsing, engaging with ads), and some are absorbing (end states like Conversion or Drop-off/Null). Once a customer reaches an absorbing state, they stay there.\n\n## Business interpretation\n\n- High conversion probability states = strong leverage points. Invest more here.\n- Low conversion probability states = weak links. Maybe redesign or reduce spend.\n- Journey insights: If “Social” has low conversion probability but often leads to “Email”, it might still be valuable as an assist channel.\n\n### What does “Email = 0.232569” mean?\nIt means:  \n**If a customer is currently in the “Email” step of the journey, there is about a 23% chance they will eventually convert (reach the Conversion state), considering all possible paths they might take from here.**\n\n## Why This Method Beats First-Touch or Last-Touch Attribution\nTraditional attribution models like first-touch (crediting the first interaction) or last-touch (crediting the final interaction) oversimplify the customer journey. They ignore the fact that most conversions happen after multiple interactions across channels.\nOur approach uses an absorbing Markov chain, which:\n\n- Considers the entire journey, not just the start or end.\n- Accounts for all possible paths and loops a customer might take before converting or dropping off.\n- Produces probabilities based on actual behavior, rather than arbitrary credit rules.\n\nBusiness impact:\n\n- You see which channels truly influence conversion—even if they’re not the first or last touch.\n- You can identify assist channels that play a critical role in moving customers forward.\n- Budget decisions become data-driven, focusing on steps that increase the likelihood of conversion."
  },
  {
   "cell_type": "markdown",
   "id": "5ae10f28-29df-4a2b-babb-44e6122b434e",
   "metadata": {
    "name": "CHANNEL_ATTRIBUTION",
    "collapsed": false
   },
   "source": "# Channel Attribution Percent:\n- These percentages are each channel’s share of the total, positive impact on conversions, measured by a “removal effect.”\n- In plain terms: If we took a channel away and re-routed the journey based on observed behavior, how much would total conversions drop? The bigger the drop, the higher the attribution %.\n- A **causal‑ish**, scenario-based measure of incremental contribution. It re-computes expected conversions after “removing” a channel (with probabilities re-allocated) and measures the drop.\n\n## What `attribution_pct` Is\n`attribution_pct` is a pandas Series that gives the percentage contribution of each channel (non-start, non-absorbing state) to total expected conversions, based on removal effects. It’s computed by:\n\n1. Measuring how much total expected conversions would drop if a channel were removed (its removal effect score).\n2. Normalizing those scores to percentages so they sum to 100% (when the total effect is positive).\n\nThis is a widely used approach in Markov chain attribution: **a channel’s importance is proportional to how much conversions would decrease if the channel didn’t exist in the journey.**"
  }
 ]
}